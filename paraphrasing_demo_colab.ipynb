{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fe6dfd",
   "metadata": {},
   "source": [
    "# üß† Transformer Paraphrasing (Colab Demo)\n",
    "Colab notebook for running inference using a custom Transformer-based paraphrasing model.\n",
    "\n",
    "---\n",
    "**Features:**\n",
    "- Loads model & vocab from GitHub using `gdown`\n",
    "- Runs beam search decoding\n",
    "- Supports token-level attention\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c655db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install required libraries\n",
    "!pip install -q nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4534d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Imports and global variables\n",
    "import os, torch, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "MAX_LEN = 20\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GLOVE_PATH = \"glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî£ Tokenization and GloVe loading\n",
    "def tokenize(text): return word_tokenize(text.lower())\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(w, vocab['<unk>']) for w in tokenize(sentence)]\n",
    "\n",
    "def load_glove_embeddings(vocab):\n",
    "    if not os.path.exists(GLOVE_PATH):\n",
    "        print(\"‚è¨ Downloading GloVe embeddings...\")\n",
    "        urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", \"glove.zip\")\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(\"glove.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "    matrix = np.random.uniform(-0.1, 0.1, (len(vocab), EMBED_DIM))\n",
    "    matrix[vocab['<pad>']] = 0\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            w, *vec = line.strip().split()\n",
    "            if w in vocab:\n",
    "                matrix[vocab[w]] = np.array(vec, dtype=np.float32)\n",
    "    return torch.tensor(matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Transformer model class\n",
    "class TransformerParaphraser(nn.Module):\n",
    "    def __init__(self, input_vocab, target_vocab, emb1, emb2):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding.from_pretrained(emb1, freeze=False)\n",
    "        self.tgt_embed = nn.Embedding.from_pretrained(emb2, freeze=False)\n",
    "        self.pos_enc = nn.Parameter(torch.rand(1, MAX_LEN, EMBED_DIM))\n",
    "        self.tr = nn.Transformer(d_model=EMBED_DIM, nhead=4, num_encoder_layers=2, num_decoder_layers=2,\n",
    "                                dim_feedforward=512, batch_first=True)\n",
    "        self.fc = nn.Linear(EMBED_DIM, len(target_vocab))\n",
    "        self.pad_idx = target_vocab['<pad>']\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = src == self.pad_idx\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(DEVICE)\n",
    "        src = self.src_embed(src) + self.pos_enc[:, :src.size(1)]\n",
    "        tgt = self.tgt_embed(tgt) + self.pos_enc[:, :tgt.size(1)]\n",
    "        out = self.tr(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_mask)\n",
    "        return self.fc(out)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load vocab and model from GitHub (via raw download)\n",
    "!curl -L -o vocab.pkl https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/vocab.pkl\n",
    "!curl -L -o paraphrase_model.pt https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/paraphrase_model.pt\n",
    "\n",
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "    input_vocab, target_vocab = pickle.load(f)\n",
    "idx2word = {i: w for w, i in target_vocab.items()}\n",
    "input_emb = load_glove_embeddings(input_vocab)\n",
    "target_emb = load_glove_embeddings(target_vocab)\n",
    "model = TransformerParaphraser(input_vocab, target_vocab, input_emb, target_emb).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"paraphrase_model.pt\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536674de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Beam decoding\n",
    "def beam_decode(model, sentence, input_vocab, target_vocab, idx2word, beam_width=5, max_len=MAX_LEN, alpha=0.7):\n",
    "    model.eval()\n",
    "    src_tokens = sentence_to_indices(sentence, input_vocab)\n",
    "    src_tensor = torch.tensor(src_tokens[:MAX_LEN] + [input_vocab['<pad>']] * (MAX_LEN - len(src_tokens))).unsqueeze(0).to(DEVICE)\n",
    "    src_mask = (src_tensor == input_vocab['<pad>'])\n",
    "    memory = model.src_embed(src_tensor) + model.pos_enc[:, :src_tensor.size(1)].to(DEVICE)\n",
    "    memory = model.tr.encoder(memory, src_key_padding_mask=src_mask)\n",
    "    beams = [(torch.tensor([target_vocab['<sos>']], device=DEVICE), [], 0.0)]\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for tokens, words, score in beams:\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tokens.size(0)).to(DEVICE)\n",
    "            tgt_emb = model.tgt_embed(tokens.unsqueeze(0)) + model.pos_enc[:, :tokens.size(0)]\n",
    "            decoder_out = model.tr.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc(decoder_out[:, -1])\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            topk = torch.topk(log_probs, beam_width)\n",
    "            for i in range(beam_width):\n",
    "                idx = topk.indices[0][i].item()\n",
    "                word = idx2word.get(idx, '<unk>')\n",
    "                new_score = score + topk.values[0][i].item()\n",
    "                if word == '<eos>':\n",
    "                    return ' '.join(words)\n",
    "                new_tokens = torch.cat([tokens, torch.tensor([idx], device=DEVICE)])\n",
    "                new_beams.append((new_tokens, words + [word], new_score))\n",
    "        beams = sorted(new_beams, key=lambda x: x[2] / ((5 + len(x[1])) ** alpha / 6**alpha), reverse=True)[:beam_width]\n",
    "    return ' '.join(beams[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Try out paraphrasing\n",
    "while True:\n",
    "    sentence = input(\"\\nEnter sentence to paraphrase (or type 'exit'): \")\n",
    "    if sentence.lower().strip() == 'exit': break\n",
    "    print(\"Paraphrase:\", beam_decode(model, sentence, input_vocab, target_vocab, idx2word))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
