{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61fe6dfd",
      "metadata": {
        "id": "61fe6dfd"
      },
      "source": [
        "# üß† Transformer Paraphrasing (Colab Demo)\n",
        "Colab notebook for running inference using a custom Transformer-based paraphrasing model.\n",
        "\n",
        "---\n",
        "**Features:**\n",
        "- Loads model & vocab from GitHub using `gdown`\n",
        "- Runs beam search decoding\n",
        "- Supports token-level attention\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "60c655db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c655db",
        "outputId": "5de2170b-980c-4f6c-e737-513591d87537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# üì¶ Install required libraries\n",
        "!pip install -q nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a4534d6b",
      "metadata": {
        "id": "a4534d6b"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Imports and global variables\n",
        "import os, torch, pickle\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "MAX_LEN = 20\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GLOVE_PATH = \"glove.6B.100d.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3d43ef80",
      "metadata": {
        "id": "3d43ef80"
      },
      "outputs": [],
      "source": [
        "# üî£ Tokenization and GloVe loading\n",
        "def tokenize(text): return word_tokenize(text.lower())\n",
        "\n",
        "def sentence_to_indices(sentence, vocab):\n",
        "    return [vocab.get(w, vocab['<unk>']) for w in tokenize(sentence)]\n",
        "\n",
        "def load_glove_embeddings(vocab):\n",
        "    if not os.path.exists(GLOVE_PATH):\n",
        "        print(\"‚è¨ Downloading GloVe embeddings...\")\n",
        "        urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", \"glove.zip\")\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(\"glove.zip\", \"r\") as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "    matrix = np.random.uniform(-0.1, 0.1, (len(vocab), EMBED_DIM))\n",
        "    matrix[vocab['<pad>']] = 0\n",
        "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            w, *vec = line.strip().split()\n",
        "            if w in vocab:\n",
        "                matrix[vocab[w]] = np.array(vec, dtype=np.float32)\n",
        "    return torch.tensor(matrix, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ba2d4080",
      "metadata": {
        "id": "ba2d4080"
      },
      "outputs": [],
      "source": [
        "# üß† Transformer model class\n",
        "class TransformerParaphraser(nn.Module):\n",
        "    def __init__(self, input_vocab, target_vocab, emb1, emb2):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding.from_pretrained(emb1, freeze=False)\n",
        "        self.tgt_embed = nn.Embedding.from_pretrained(emb2, freeze=False)\n",
        "        self.pos_enc = nn.Parameter(torch.rand(1, MAX_LEN, EMBED_DIM))\n",
        "        self.tr = nn.Transformer(d_model=EMBED_DIM, nhead=4, num_encoder_layers=2, num_decoder_layers=2,\n",
        "                                dim_feedforward=512, batch_first=True)\n",
        "        self.fc = nn.Linear(EMBED_DIM, len(target_vocab))\n",
        "        self.pad_idx = target_vocab['<pad>']\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = src == self.pad_idx\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(DEVICE)\n",
        "        src = self.src_embed(src) + self.pos_enc[:, :src.size(1)]\n",
        "        tgt = self.tgt_embed(tgt) + self.pos_enc[:, :tgt.size(1)]\n",
        "        out = self.tr(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_mask)\n",
        "        return self.fc(out)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "107f62df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "107f62df",
        "outputId": "63e216fb-89a0-4a1b-a21c-4181e4edf1d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Failed to download vocab.pkl: status code 401",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-168553e56bd7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download {filename}: status code {r.status_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msafe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/vocab.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocab.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msafe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/paraphrase_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paraphrase_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-168553e56bd7>\u001b[0m in \u001b[0;36msafe_download\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{filename} is empty!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download {filename}: status code {r.status_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msafe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/vocab.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocab.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Failed to download vocab.pkl: status code 401"
          ]
        }
      ],
      "source": [
        "# üì• Safe GitHub download with verification\n",
        "import requests\n",
        "\n",
        "def safe_download(url, filename):\n",
        "    try:\n",
        "        r = requests.get(url, allow_redirects=True, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            # Additional check: try loading to validate pickle file\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filename, 'rb') as test_f:\n",
        "                    pickle.load(test_f)\n",
        "        else:\n",
        "            raise Exception(f\"Download failed: {url}, status code {r.status_code}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Download or verification failed for {filename}: {str(e)}\")\n",
        "\n",
        "safe_download(\"https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/vocab.pkl\", \"vocab.pkl\")\n",
        "safe_download(\"https://huggingface.co/datasets/shubham27eu/paraphraser/resolve/main/paraphrase_model.pt\", \"paraphrase_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"vocab.pkl\", \"rb\") as f:\n",
        "    input_vocab, target_vocab = pickle.load(f)\n",
        "idx2word = {i: w for w, i in target_vocab.items()}\n",
        "input_emb = load_glove_embeddings(input_vocab)\n",
        "target_emb = load_glove_embeddings(target_vocab)\n",
        "model = TransformerParaphraser(input_vocab, target_vocab, input_emb, target_emb).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"paraphrase_model.pt\", map_location=DEVICE))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "TO7TV3nK4T5w"
      },
      "id": "TO7TV3nK4T5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536674de",
      "metadata": {
        "id": "536674de"
      },
      "outputs": [],
      "source": [
        "# üîÅ Beam decoding\n",
        "def beam_decode(model, sentence, input_vocab, target_vocab, idx2word, beam_width=5, max_len=MAX_LEN, alpha=0.7):\n",
        "    model.eval()\n",
        "    src_tokens = sentence_to_indices(sentence, input_vocab)\n",
        "    src_tensor = torch.tensor(src_tokens[:MAX_LEN] + [input_vocab['<pad>']] * (MAX_LEN - len(src_tokens))).unsqueeze(0).to(DEVICE)\n",
        "    src_mask = (src_tensor == input_vocab['<pad>'])\n",
        "    memory = model.src_embed(src_tensor) + model.pos_enc[:, :src_tensor.size(1)].to(DEVICE)\n",
        "    memory = model.tr.encoder(memory, src_key_padding_mask=src_mask)\n",
        "    beams = [(torch.tensor([target_vocab['<sos>']], device=DEVICE), [], 0.0)]\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for tokens, words, score in beams:\n",
        "            tgt_mask = model.generate_square_subsequent_mask(tokens.size(0)).to(DEVICE)\n",
        "            tgt_emb = model.tgt_embed(tokens.unsqueeze(0)) + model.pos_enc[:, :tokens.size(0)]\n",
        "            decoder_out = model.tr.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
        "            logits = model.fc(decoder_out[:, -1])\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "            topk = torch.topk(log_probs, beam_width)\n",
        "            for i in range(beam_width):\n",
        "                idx = topk.indices[0][i].item()\n",
        "                word = idx2word.get(idx, '<unk>')\n",
        "                new_score = score + topk.values[0][i].item()\n",
        "                if word == '<eos>':\n",
        "                    return ' '.join(words)\n",
        "                new_tokens = torch.cat([tokens, torch.tensor([idx], device=DEVICE)])\n",
        "                new_beams.append((new_tokens, words + [word], new_score))\n",
        "        beams = sorted(new_beams, key=lambda x: x[2] / ((5 + len(x[1])) ** alpha / 6**alpha), reverse=True)[:beam_width]\n",
        "    return ' '.join(beams[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cc4352",
      "metadata": {
        "id": "08cc4352"
      },
      "outputs": [],
      "source": [
        "# üß™ Try out paraphrasing\n",
        "while True:\n",
        "    sentence = input(\"\\nEnter sentence to paraphrase (or type 'exit'): \")\n",
        "    if sentence.lower().strip() == 'exit': break\n",
        "    print(\"Paraphrase:\", beam_decode(model, sentence, input_vocab, target_vocab, idx2word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9684a23",
      "metadata": {
        "id": "f9684a23"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/vaibhavkl/Quora-Question-Pairs-Dataset/master/quora_duplicate_questions.tsv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"quora_duplicate_questions.tsv\", sep='\\t')\n",
        "df = df[['question1', 'question2', 'is_duplicate']].dropna()\n",
        "pairs = [(q1, q2) for q1, q2, dup in df.values if dup == 1]\n",
        "print(\"Total duplicate pairs:\", len(pairs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f87085b",
      "metadata": {
        "id": "8f87085b"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(tokenize(s))\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "all_q1 = [q1 for q1, _ in pairs]\n",
        "all_q2 = [q2 for _, q2 in pairs]\n",
        "input_vocab = build_vocab(all_q1)\n",
        "target_vocab = build_vocab(all_q2)\n",
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump((input_vocab, target_vocab), f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b51e91",
      "metadata": {
        "id": "f9b51e91"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ParaphraseDataset(Dataset):\n",
        "    def __init__(self, pairs, input_vocab, target_vocab):\n",
        "        self.pairs = pairs\n",
        "        self.input_vocab = input_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        q1, q2 = self.pairs[idx]\n",
        "        x = sentence_to_indices(q1, self.input_vocab)\n",
        "        y = [self.target_vocab['<sos>']] + sentence_to_indices(q2, self.target_vocab) + [self.target_vocab['<eos>']]\n",
        "        x = x[:MAX_LEN] + [self.input_vocab['<pad>']] * (MAX_LEN - len(x))\n",
        "        y = y[:MAX_LEN] + [self.target_vocab['<pad>']] * (MAX_LEN - len(y))\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "train_data = ParaphraseDataset(pairs[:50000], input_vocab, target_vocab)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94ffd88",
      "metadata": {
        "id": "b94ffd88"
      },
      "outputs": [],
      "source": [
        "input_emb = load_glove_embeddings(input_vocab)\n",
        "target_emb = load_glove_embeddings(target_vocab)\n",
        "\n",
        "model = TransformerParaphraser(input_vocab, target_vocab, input_emb, target_emb).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=target_vocab['<pad>'])\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        tgt_inp = y[:, :-1]\n",
        "        tgt_out = y[:, 1:]\n",
        "\n",
        "        pred = model(x, tgt_inp)\n",
        "        pred = pred.reshape(-1, pred.shape[-1])\n",
        "        tgt_out = tgt_out.reshape(-1)\n",
        "\n",
        "        loss = criterion(pred, tgt_out)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f01919",
      "metadata": {
        "id": "01f01919"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"paraphrase_model.pt\")\n",
        "print(\"‚úÖ Saved vocab.pkl and paraphrase_model.pt!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}